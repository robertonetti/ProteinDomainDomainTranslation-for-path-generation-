{
    "num_encoder_layers": 5,
    "num_decoder_layers": 5,
    "batch_size": 512,
    "num_heads": 2,
    "forward_expansion": 2048,
    "Unalign": false,
    "num_epochs": 5000,
    "wd": 0.0,
    "src_vocab_size": 21,
    "trg_vocab_size": 21,
    "embedding_size": 106,
    "alpha_entropic": 0.0,
    "use_entropic": false,
    "learning_rate": 5e-05,
    "dropout": 0.1
}